{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NUV62myocgaR"
      },
      "outputs": [],
      "source": [
        "#https://www.pinecone.io/\n",
        "#https://app.pinecone.io/organizations/-NgXykLJD3aeQmvj62TG/projects/6cbf827f-8f37-433f-b0ad-72747bb8e36d/indexes?sessionType=signup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install pinecone-client==2.2.4\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1DLgXWCpAQF",
        "outputId": "358fb73b-04f4-4c9d-9aa5-411f0b3ba2be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.30 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.31)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.38)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.38)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.37->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: pinecone-client==2.2.4 in /usr/local/lib/python3.10/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (6.0.1)\n",
            "Requirement already satisfied: loguru>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (4.10.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client==2.2.4) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.4) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.4) (2024.2.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers==2.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRRkElbhpbI2",
        "outputId": "d8430df8-5dbf-4f8a-9045-c972f70434f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3elJmKSsrz0Z",
        "outputId": "f4e768e3-df74-4f32-e316-696fe3530632"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘pdfs’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "TJBXYXJwpE6o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFJnhHezvzK2",
        "outputId": "86b35e9b-d3e4-41c3-ea2f-6663ff15c753"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='YOLO-Z: Improving small object detection in YOLOv5 for\\nautonomous vehicles\\nAduen Benjumea* Izzeddin Teeti†Fabio Cuzzolin†Andrew Bradley*\\n17065125@brookes.ac.uk, 19136994@brookes.ac.uk,\\nfabio.cuzzolin@brookes.ac.uk, abradley@brookes.ac.uk\\nAbstract\\nAs autonomous vehicles and autonomous racing rise in popularity, so does the need for faster and more accurate\\ndetectors. While our naked eyes are able to extract contextual information almost instantly, even from far away,\\nimage resolution and computational resources limitations make detecting smaller objects (that is, objects that occupy\\na small pixel area in the input image) a genuinely challenging task for machines and a wide-open research ﬁeld. This\\nstudy explores how the popular YOLOv5 object detector can be modiﬁed to improve its performance in detecting\\nsmaller objects, with a particular application in autonomous racing. To achieve this, we investigate how replacing\\ncertain structural elements of the model (as well as their connections and other parameters) can affect performance\\nand inference time. In doing so, we propose a series of models at different scales, which we name ‘YOLO-Z’, and\\nwhich display an improvement of up to 6.9% in mAP when detecting smaller objects at 50% IOU, at the cost of\\njust a 3ms increase in inference time compared to the original YOLOv5. Our objective is to inform future research\\non the potential of adjusting a popular detector such as YOLOv5 to address speciﬁc tasks and provide insights on\\nhow speciﬁc changes can impact small object detection. Such ﬁndings, applied to the broader context of autonomous\\nvehicles, could increase the amount of contextual information available to such systems.\\n1 Introduction\\nDetecting small objects in images can be challenging, mainly due to limited resolution and context information avail-\\nable to a model [2]. Many modern systems that implement object detection do so at real-time speeds, setting speciﬁc\\nrequirements in computational resources, especially if the processing is to happen on the same device that captures\\nthe images. This is the case for many autonomous vehicle systems [4], where the vehicle itself captures and processes\\nimages in real-time, often to inform its next actions. In this context, detecting smaller objects means detecting objects\\nfarther away from the car, thus allowing earlier detection of such objects, effectively expanding the detection range of\\nthe vehicle. Improvements in this speciﬁc area would better inform the system, allowing it to make more robust and\\nviable decisions.\\nDue to the nature of object detectors, the details of smaller objects lose signiﬁcance as they are processed by each\\nlayer of their convolutional backbone. In this study, by ‘small objects’, we refer to objects which occupy a small pixel\\narea in the input image.\\nEfforts have been made to improve the detection of smaller objects [19], but many revolve around directing the pro-\\ncessing around a speciﬁc area of the image [29, 28, 27] or are focused around two-stage detectors, which are known for\\nachieving better performance at the cost of inference time, making them less suited for real-time applications. This is\\nalso the reason why so many single-stage detectors have been developed for this type of applications [31]. Increasing\\nthe input image resolution is another obvious way to bypass this issue which results, however, in a signiﬁcant increase\\nin processing time.\\nYOLOv5 is a very popular single-stage object detector [11] known for its performance and speed with a clear and\\nﬂexible structure that can be broken down, adjusted and built on a very widely accessible platform. Many of the\\nsystems that apply this architecture and attempt to optimise it, however, they mainly rely on adjusting speciﬁc param-\\neters or augmenting their training set to improve performance [33], without much consideration for structural changes\\n*Visual Artiﬁcial Intelligence Laboratory, Oxford Brookes University, UK\\n†Autonomous Driving & Intelligent Transport Group, Oxford Brookes University, UK\\n1arXiv:2112.11798v4  [cs.CV]  3 Jan 2023', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 0}),\n",
              " Document(page_content='to the model itself to better adapt it for a speciﬁc use case. While YOLOv5 is a potent tool, it is designed to be a\\ngeneral-purpose object detector and therefore is not optimised to detect smaller objects.\\nThis study proposes ways in which YOLOv5 can be modiﬁed to better perform on a given system in terms of small\\nobject detection, with clear real-world implications [4]. We consider, in particular, the case of an autonomous racing\\nvehicle that needs to detect differently coloured cones to drive around a track. We will discuss the effects of differ-\\nent techniques and propose modiﬁed models able to perform this task better while maintaining real-time processing\\nspeeds. The contributions of this paper are:\\n1. A modiﬁed model of YOLOv5 speciﬁcally designed for better detections of small objects.\\n2. Proposing a methodology to modify the structure of YOLOv5 to improve performance in a particular task. This\\nis done in an experimental manner, analysing the different elements that make YOLOv5.\\n2 Related work\\nThis study aims at reﬁning the already existing YOLOv5 model to deal with the many problems associated with\\nsmall object detection. This task is a complex area of machine learning that very quickly escalates in complexity as\\nrequirements evolve. To work with such systems, it is essential to understand the bases upon which they are built, the\\nmany different technologies and techniques that form the current state of the art and the related use cases.\\n2.1 One-stage vs two-stage objects detectors\\nWe know we can classify object detectors into two categories: one-stage and two-stage detectors [19]. The latter\\ntypically decomposes the detection task into (i) region proposal generation and (ii) classiﬁcation, as is the case with\\nFaster R-CNN and its predecessors [6, 5, 26]. While there have been efforts to improve the small object detection\\nability of such models [3], a lot of the attention has been put on performance regardless of inference time. Two stage\\ndetectors have however improved signiﬁcantly over time by streamlining their structure and data ﬂow.\\n2.2 The YOLO family\\nAs a family of object detectors, YOLO takes this idea a step further and has grown very popular over the last few years.\\nWith YOLOv1 [23], object detection is presented as a regression task, thus simplifying the networks and allowing us\\nto build faster models that can be used in real-time. Later versions of YOLO improve different aspects of the model\\n[23, 24, 25, 1]. Most notably, much effort has been spent on the backbone through the different versions. This begs\\nthe question: What potential is there still untapped if changes to an isolated element can have such an impact?\\nYOLOv5 [11] was released very shortly after YOLOv4 [1]. Despite its name, the authors are not directly related,\\nand there have been discussions on whether it is fair to call YOLOv5 a successor of YOLOv4. This implementation\\nprovides similar performance to YOLOv4 and shares the same design. The main point of attention is the fact that it\\nis fully written in the PyTorch framework [20] as opposed to using any form of the Darknet framework [22] and has\\na focus on accessibility and use in a wider range of development environments. Additionally, the models in YOLOv5\\nprove to be signiﬁcantly smaller, faster to train and more accessible to be used in a real-world application.\\n2.3 Systems using and modifying YOLOv5\\nYOLO has been used in many applications requiring the detection of objects. In safety helmet detection systems\\n[34], for instance, YOLO can be adjusted and implemented in series with the rest of a system. Similarly, face masks\\ndetectors have been seen at the entrances of metro stations [33]. Both of these applications do a good job at exploiting\\nthe beneﬁts of YOLO for the detection of smaller objects [21], but do not go as far as modifying the architecture.\\nOther systems that do make an effort to optimize YOLOv5 do so in a limited fashion. Once again, mask detectors [16]\\nhave been proposed that leverage anchors generated and data augmentation to ﬁt a model to the use case better. More', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 1}),\n",
              " Document(page_content='complex systems for helmet detection [10] also do a great job at leveraging the contextual information around small\\nobjects to isolate them and facilitate their detection. However, their approach is not quite universally applicable and\\ncomes at the cost of introducing a two-step process.\\nTypical adjustments to the internal structures of the model are surface-level. In a recent apple detection system [32],\\nthe backbone of YOLOv5 is slightly modiﬁed to simplify it, which offers the potential to adapt to the system’s re-\\nquirements and one that opens the way for additional changes. If a single backbone element is modiﬁed, more drastic\\nchanges can be applied for additional effects.\\n2.4 Small object detection\\nSome effort has been put into developing systems which direct the processing towards certain areas of the input image\\n[29, 28, 27], which allows us to adjust resolution and therefore bypass the limitation of having fewer pixels deﬁning an\\nobject. This approach, however, is better suited for systems that are not time-sensitive, as they require multiple passes\\nthrough a network at different scales. This idea of paying more attention to speciﬁc scales can nevertheless inspire the\\nway we treat certain feature maps.\\nAdditionally, a lot can be learned by looking at how feature maps can be treated instead of just modifying the backbone.\\nDifferent types of feature pyramid networks (FPN) [13, 30, 15] can aggregate feature maps differently to enhance a\\nbackbone in different ways. Such techniques prove to be rather effective.\\n2.5 Autonomous vehicles\\nWithin autonomous driving, object detection can provide valuable contextual information about the vehicle’s surround-\\nings and heavily inform its decision making process [17, 4]. In this case, smaller objects translate to objects further\\naway, meaning a more complete context for the system to make use of. These systems heavily focus on inference time,\\nsacriﬁcing performance if needed, but work can be done to improve them at minimal cost. Performance in this ﬁeld is\\ncritical, as a small improvement in this system can greatly impact the entire vehicle. A common requirement in this\\narea is for detectors to be single-stage [31], for the simple reason that fewer steps and transitions between them often\\ntranslates into fewer resources needed.\\n3 Methodology\\nYOLOv5 provides four different scales for their model, S,M,LandXwhich stand for Small, Medium, Large, and\\nXlarge, respectively. Each of these scales applies a different multiplier to the depth and width of the model, meaning\\nthe overall structure of the model remains constant, but the size and complexity of each model are scaled. In Our\\nexperiments, we apply changes to the structure of the models individually across all the scales and treat each one as a\\ndifferent model for the purposes of evaluating their effect.\\nTo set a baseline, we trained and tested the unmodiﬁed versions of the four scales of YOLOv5. We then tested changes\\nto these networks individually in order to observe their impact separately against our baseline results. The techniques\\nand structures that did not appear to contribute to better accuracy or inference time were ﬁltered out when moving to the\\nnext phase. We then attempted combinations of the selected techniques. This process was repeated, observing whether\\ncertain techniques complemented or diminished each other and adding more complex combinations progressively.\\nWe ﬁrst discuss the appropriate evaluation metric for our work (Section 3.1), and the dataset used for our investigation\\n(Section 3.2). We then move on to describe our plans to apply a number of model changes to be run under controlled\\ncircumstances (Section 3.2), logging and adjusting as we move through different stages.\\n3.1 Evaluation metric\\nThe original implementation of YOLOv5 provides compatibility with Microsoft Common Objects in Context (COCO)\\nAPI’s [14] metrics at three different object scales (bounding box areas) and Intersection over Unions (IOU ), which\\nproves useful for the purpose of this study. The way values at speciﬁc scales are calculated can give us a good indication', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 2}),\n",
              " Document(page_content='of the performance of the model, but may be slightly inaccurate in extreme cases, which will not be a problem for the\\nmost part, but must be kept in mind.\\nSince these metrics are only compatible with the COCO dataset by default, we have re-implemented this method in\\nour testing code in order to obtain more valuable ﬁgures for our study while using any dataset. Our metric module\\nwill calculate values for large ,medium andsmall objects, in addition to the overall performance. The categorisation\\nof objects into these three categories depends on the following thresholds: ‘small’, if the object occupy an area less\\nthan 32 squared pixels, ‘large’, if the area is more than 96 squared pixels, and ‘medium’, for anything between the two\\nthresholds. In other words, small <322<medium <962<large.\\n3.2 Dataset and Experimental setup\\nFigure 1: Dataset class instancesTo train our models and inform our experiments we adopted\\na dataset of annotated cones from the perspective of an au-\\ntonomous racing car. Its original purpose is to help plan a path\\nfor an autonomous racing vehicle based on the colours of the\\ncones, knowing that there are a total of 4 classes (yellow, blue,\\norange and big orange cones) and close to 4,000 images (see\\nFigure 1, 2). This dataset includes digitally augmented images\\n[18] and cases with challenging weather conditions. A dataset\\nsuch as this one models more complex tasks in autonomous\\nvehicles. Cones are themselves objects that we would ﬁnd on\\nthe road and share many qualities with other objects such as of\\ntrafﬁc signs in terms of size and position.\\nAlthough the dataset would beneﬁt from a larger size, it is\\ncharacterised by a very high object density, with over 30,000\\nlabelled objects. Furthermore, looking at Figure 1, we can ob-\\nserve a very high bias towards the blue and yellow classes.\\nThis makes sense as they serve to mark the two sides of the\\nracing track, but it does constitute an imbalance that will af-\\nfect the overall results (see Section 4). The performance on\\nthese classes will be taken into account when evaluating the\\nmodels, namely by averaging the scores of the most prominent\\nclasses.\\nCones are naturally small objects already in comparison to\\nother objects commonly found in the autonomous driving scenario, such as other vehicles or pedestrians. The correl-\\nogram (a chart of correlation statistics) in Figure 3 shows the position, width, and height of the bounding boxes of the\\nobjects (cones) in the dataset. Our dataset features a high concentration of smaller object boxes, slightly elongated as\\nto be expected because of perspective projection. This high proportion of small objects makes it beneﬁcial for this type\\nof study, as it largely overcomes the issue with a lack of such objects in other popular datasets including MS COCO\\n[12].\\nFigure 2: Sample image from datasetThe dataset was split into training, validation and testing with\\na ratio of 65:15:20. The validation set then informs the training\\nof the model, but is not as relevant as the other two, hence the\\nlower size.\\nThe training for all the experiments was executed in an envi-\\nronment that has 4 Nvidia GTX 1080 GPUs, each has 12 GB\\nVRAM. For testing, however, we used a single GTX1080TI\\nGPU with a batch size of 1, and an i7-6900K CPU working at\\n3.20GHz.', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 3}),\n",
              " Document(page_content='3.3 Proposed architectural changes\\nFigure 3: Relation between the position (in x and y value of the center\\npoint), width and height of instances of the dataset\\nFigure 4: YOLOv5 default structure. In the text we refer to\\nthe elements of this architecture modiﬁed in our work.YOLOv5 uses a yaml ﬁle to instruct a parser how to\\nbuild a model. We use this setup to write our own high-\\nlevel instructions on how different building blocks of the\\nmodel are built and with what parameters, hence modify-\\ning its structure. to implement new structures we arrange\\nand give parameters to each building block or layer and\\ninstruct the parser on how to build it if necessary. In our\\nwords, we make use of the base and experimental net-\\nwork blocks provided with YOLOv5, while implement-\\ning additional blocks where needed to simulate the re-\\nquired structures.\\n3.3.1 Backbone\\nThe backbone of a model is the element dedicated to tak-\\ning the input image and extracting feature maps from it.\\nThis is a crucial step in any object detector, as it is the\\nmain structure responsible for extracting contextual in-\\nformation from the input image as well as for abstracting\\nthat information into patterns. We experimented with re-\\nplacing the existing backbone in YOLOv5 with two sep-\\narate options. ResNet [8] is a popular structure that intro-\\nduces residual connections to lessen the effect of the di-\\nminishing return we observe in deeper neural networks.\\nDenseNet [9] uses similar connections to preserve as\\nmuch information as possible as it moves through the\\nnetwork. Implementing these structures requires break-\\ning them down to their fundamental blocks and ensuring\\nthe layers communicate appropriately. This includes en-\\nsuring the right feature map dimensions, which at times\\nrequires slightly modifying the scaling factor for the\\nwidth and depth of the model.\\nIn both cases, it was important to avoid drastically devi-\\nating the number of layers from the original as to main-\\ntain a comparable complexity. Hence, ResNet50 was\\nused and we downscaled DenseNet proportionally so it reattains its core functionality.\\nAdditionally, YOLOv5 makes use of a Spatial Pyramid Pooling (SPP) [7] layer in between the backbone and the neck.\\nIn our work, however, we have maintained this layer untouched.\\n3.3.2 Neck\\nWe term ‘neck’ the structure placed between the head and backbone (see Figure 4) whose objective is to aggregate as\\nmuch information extracted by the backbone as possible before it is fed to the head. This structure plays a major role\\nin transferring small-object information by preventing it from being lost to higher levels of abstraction. It does this by\\nupsampling the resolution of the feature maps once again so different layers from the backbone can be aggregated and\\nregain inﬂuence on the detection step. [15].\\nIn this work, we simpliﬁed the current Pan-Net [15] to that of an FPN [13] and replaced it a with biFPN [30]. In\\nboth cases the neck retains a similar functionality, but varies in complexity and therefore the number of layers and\\nconnections required for their implementation.', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 4}),\n",
              " Document(page_content='3.3.3 Other modiﬁcations\\nFigure 5: Example of how favoring smaller feature maps can be implemented\\nin terms of structure both inclusively and exclusively.The head of the model is responsible for taking\\nfeature maps and inferring the bounding boxes\\nand classes by taking in several aggregated feature\\nmaps from the neck. This structure can remain un-\\ntouched, other than the parameters it receives, as\\nit is a fundamental part of the model that does not\\nhave as much impact in small object detection as\\nthe aforementioned elements.\\nThere are, however, other elements that can have\\nan impact on small object detection performance.\\nOther than input image size, the depth and width\\nof the model can be modiﬁed in order to change\\nwhat aspect of the network the bulk of the process-\\ning goes towards. The way layers are connected\\ncan also be manually altered in the neck and head\\nin order to focus on detecting certain feature maps.\\nIn this study we explored the effect of redirecting\\nthe connections involving higher-resolution fea-\\nture maps, in order for them to be fed directly to\\nthe neck and head. This can be done in an ‘inclu-\\nsive’ manner by expanding the neck to ﬁt an extra\\nfeature map, or in an ‘exclusive’ fashion by replac-\\ning the lowest-resolution feature map in order to ﬁt\\nthe new one, Figure 5 shows both options, as well\\nas the default (original) layout. Making use of a\\nhigher resolution feature map would usually im-\\nprove performance on smaller object at the cost of\\ninference time and potentially detection of larger\\nobjects, similar to the effect of increasing input im-\\nage size. We integrate this behaviour in the neck in\\nthese two ways to minimize the downsides while\\nmaking the most out of its beneﬁts.\\nNote that a number of parameters will have to be adjusted to the new structure, as the learning capabilities of the\\nnetwork can be affected. Mainly, the sizes of the anchor boxes applied in the head, which need to adjust to the\\nresolution of the feature maps being used.', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 5}),\n",
              " Document(page_content='Figure 6: Results of applying individual architectural changes to YOLOv5 at each scale. We report the mAp at 50% IOU across all objects\\nsizes (top), the mAP at 50% IOU for small objects only (middle), and the inference speed in frames per second (fps, bottom). YOLOv5 : the\\nbaseline. lr02: changing the learning rate to 0.02. lr005 : changing the learning rate to 0.005. resnet50 : changing the backbone to that of ResNet50.\\ndensenet : changing the backbone to that of DenseNet. 3anch : auto-generating 3 anchors per scale. 5anch : auto-generating 5 anchors per scale.\\nfpn: changing neck to that of FPN. bifpn : changing neck to that of BiFPN. deep : increasing the depth modiﬁer to that of the next scale up (or\\nequivalent). wide : increasing the width modiﬁer to that of the next scale up (or equivalent). XSinc: setting an extra small feature map inclusively\\n(see 3.3.3). XSex: setting an extra small feature map exclusively (see 3.3.3).\\n4 Results\\nModel Features\\nYOLO-Z S DenseNet, FPN, 3 anchors, extra small exclusive feature map\\nYOLO-Z M DenseNet, FPN, 5 anchors, extra small exclusive feature map\\nYOLO-Z L DenseNet, FPN, 5 anchors, extra small exclusive feature map\\nYOLO-Z X DenseNet, bi-FPN, 5 anchors, extra small exclusive feature map\\nTable 1: Modiﬁcations applied to YOLOv5 to achieve models of the YOLO-Z family. Each scale uses its YOLOv5 equivalent as a base.\\nmAP .5 mAP .5 small inference (ms)\\nScales YOLOv5 YOLO-Z difference YOLOv5 YOLO-Z difference YOLOv5 YOLO-Z difference\\nS 0.926 0.955 3.13% 0.869 0.925 6.44% 8 8.9 0.9\\nM 0.932 0.9605 3.06% 0.8795 0.9425 7.16% 11.6 14.3 2.7\\nL 0.935 0.964 3.10% 0.886 0.9545 7.73% 16.6 19.6 3\\nX 0.9385 0.9605 2.34% 0.8975 0.9465 5.46% 26.9 30.6 3.7\\nTable 2: Comparing performance and inference time of YOLOv5 and YOLO-Z (optimal values for each scale in bold).\\nNote that we only show performance on the yellow and blue classes, as they are the best represented in the dataset\\naccording to Figure 1. (See supplementary material ”Individual test results”, Table 1).\\n4.1 Inﬂuence of the backbone\\nA comparison of the performance of the two backbones (see Figure 6) shows that DenseNet consistently exhibits a\\nsigniﬁcant improvement at what appears to be a relatively low ﬁxed increase in inference time (about 3 ms). ResNet\\nnot only seems to worsen performance in most cases, but its inference time is also signiﬁcantly higher, leaving no\\nreason to consider it further at this stage. Our conclusion is that DenseNet is therefore a better ﬁt, in general, for small\\nscale object detection. In the smaller scale models, this can be due to not having networks deep enough to reap the\\nbeneﬁts of a ResNet backbone, while DenseNet does a good job at preserving feature maps’ details.', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 6}),\n",
              " Document(page_content='Figure 7: Performance comparison between the YOLOv5 and YOLO-Z families of models, plotting mAP (top), mAP for small objects (middle)\\nand mAP for medium objects (bottom) against inference time (ms). Clearly the superior average performance of YOLO-Z is achieved at the smaller\\nscale, while performance is stable and very close to 1 at the medium scale.\\n4.2 Inﬂuence of neck architecture\\nUsing an FPN only outperforms bi-FPN at the Sscale (see Figure 6). In the latter case inference time remains fairly\\ncomparable to that of the original YOLOv5 neck, which is not too surprising given their similarities. This might\\nsuggest that simpler models beneﬁt from keeping the feature maps relatively untouched, while other scales require\\nextra steps to adapt to the added processing of the feature maps and eventually outperform the former.\\n4.3 Feature maps\\nIn our experiments, redirecting what feature maps are fed to the neck and head had the most signiﬁcant impact among\\nall techniques. Excluding the lowest resolution feature map in order to replace it with a higher-resolution one ( XSex\\nin Figure 6) proved particularly effective. This can be attributed to the fact that, after including a higher resolution\\nmap in the head, small objects end up occupying more pixels and having therefore more of an inﬂuence, rather than\\nbeing ‘lost’ in the convolution stages of the backbone. Similarly, getting rid of the original lower-resolution feature\\nmap reduces the amount of processing needed and prevents the model from counteracting the level of detail provided\\nby the higher-resolution map. This is likely a consequence of the dataset used having a very high density of small\\nobjects; performance will likely vary signiﬁcantly in other applications. The only exception to this pattern seems to\\nbe that of the extra large scale ( X), for which the improvement is not as signiﬁcant and keeping the lower resolution\\nfeature maps actually appears detrimental to performance In comparison to the baseline.\\n4.4 Inﬂuence of the number of anchors\\nLetting YOLO generate anchors based on the dataset provided proves effective in performance without affecting\\ninference time. However, The magnitude of the effect and the number of anchors a model favors does seem to be\\naffected by scale ( anch3 andanch5 in Figure 6). Again, the dataset used is relevant to this step as, in our tests, most\\ncones will have a similar elongated shape on the y axis (see Figure 3). Other applications with objects varying more\\nin shape might ﬁnd different results.\\nIn terms of scale, the smaller models tend to beneﬁt from fewer anchors, while the opposite is true for the larger scales.\\nNamely, at the Sscale having 3 anchors outperforms having 5, while the gap reduces at the Mscale. Models Land\\nX, on the other hand, display instead a better performance at 5 anchors. This suggests that more complex or deeper\\nmodels may indeed beneﬁt from additional anchors or, in other words, may be more capable of taking advantage of\\nthe details additional anchors provide.', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 7}),\n",
              " Document(page_content='4.5 Other factors\\nFigure 8: Visual demonstration of the improved detec-\\ntion results of YOLO-Z S (bottom) compared to YOLOv5\\nS (top) over a region of a sample image covering far away /\\nsmall scale objects. Yellow and blue cone detections are\\nshown as bounding boxes in the respective colours, de-\\ntections missed by both models are shown as red boxes,\\ndetections missed by YOLOv5 but correctly identiﬁed by\\nYOLO-Z S as red circles. One can observe that the im-\\nprovement is most evident for farther away / smaller cones.In addition to the other major structural changes, a larger learning\\nrate proved to better leverage the models, but this can vary with the\\nnumber of epochs the latter are trained for ( lr02 andlr005 in Figure\\n6).\\nInterestingly, a wider model (higher width multiplier) showed to\\nhave a positive effect on the smaller scales as opposed to a deeper\\none ( deep andwide in Figure 6). The opposite is true for the L\\nscale. This might be due to the speciﬁc characteristics of our dataset,\\nin which the objects to be detected form relatively simple patterns\\nwith rather similar features. Nevertheless, more testing would be\\nneeded to determine this, as this pattern is not continued in the extra\\nlarge scale, where both modiﬁcations harm performance by the same\\namount. Additionally, these types of alterations do have a noticeable\\nnegative effect on inference speed, discouraging their use.\\n4.6 Modiﬁed models\\nAdditional tests were carried out using various combinations of the\\naforementioned alteration techniques in order to seek models that\\nfurther deviate from the originals but, at the same time, can further\\nimprove performance. We refer to this proposed family of models at\\ndifferent scales as YOLO-Z, short for ‘YOLO Zoomed’ (see Table\\n1).\\nA comparison of the performance of these new models shows that\\nan FPN neck tends to outperform bi-FPN (compare YOLO-Z X with\\nthe other YOLO-Z models in Table 2) for scales where the opposite\\nwas previously true. Aside from this, we could only observe small\\nvariations across the models. An exception is the X scale, which\\nseems to gain less from such changes and even with the use of a\\ndifferent neck structure does not deliver improvements as signiﬁcant\\nas with the other scales.\\nThis considered, YOLO-Z models achieved an average 2.7 performance increase in absolute mAP at 50% IoU for all\\nobjects and an absolute improvement of 5.9 for small objects at the same IoU across all scales. This comes at the cost\\nof an average 2.6ms increase in inference time.\\n4.7 Discussion\\nIn our investigation of ways in which a popular object detector such as YOLOv5 can be adapted to better detect smaller\\nobjects, we were able identify architectural modiﬁcations delivering a clear improvement in performance compared to\\noriginal at relatively little cost, as the new models retain real-time inference speed.\\nThe context in which we have applied the proposed techniques, that of autonomous racing, is one that can greatly\\nbeneﬁt from such an improvement. As we can see in Figure 8, such changes do have a quantiﬁable impact on detection.\\nIn this work we have not only signiﬁcantly improved the performance of the baseline model, but also identiﬁed a\\nnumber of speciﬁc techniques that can be applied to any other application involving the detection of small or far away\\nobjects.\\nThe net result is that models of the YOLO-Z family outperform those of the YOLOv5 class while retaining an inference\\ntime compatible with a real time application such as autonomous racing (see Table 2 and Figure 7). This is especially\\ntrue for the smaller objects which have been the focus of this study (Figure 7, middle), whereas the performance is\\nstable for medium-sized objects (bottom).', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 8}),\n",
              " Document(page_content='Note that, while we have focused here on modifying the popular YOLOv5 model, the methods and techniques we\\nexplored have a potential to be developed into an entirely original model structure.\\nFinally, while this study shows signiﬁcant the empirical gains of the proposed architectural changes, the consistency\\nand generality of the results could and should be further investigated. For instance, the analysis would greatly beneﬁt\\nfrom further testing with different datasets, and challenges that might for instance come from detecting trafﬁc signs.\\nWhile we have demonstrated the usefulness of the various techniques introduced, these can only be reﬁned and better\\nunderstood when applied to a varied set of circumstances and settings. Doing so would be a signiﬁcant step towards a\\nmore robust solution to small object detection. Additionally, there many more directions and techniques that would ﬁt\\nnicely in this subject and which have not been considered in this study, but these will remain a subject of future study.\\n5 Conclusions\\nIn this study, we have investigated the effects and rationale of different architectural and model alterations applied\\nto the popular YOLOv5 object detector in order to improve its small-object detection abilities. We have validated\\nthese techniques in the autonomous racing scenario, highlighting its speciﬁc needs and limitations, and outlining\\npossible further research. Doing so has produced an original YOLO-Z family of models capable of delivering an\\nimprovement in the ability of detecting small objects (measured by the standard mAP at 50% IoU) close to 6%, while\\nonly increasing inference time by around 3 ms. Using these ﬁndings existing systems can be upgraded to better detect\\nvery small objects in situation in which current models cannot detect anything at all. This can extend an autonomous\\nvehicle’s detection range and perception robustness, leading to better planning and decision making strategies that can\\ngive an autonomous racing car an important edge.\\n6 Acknowledgements\\nThanks go to the Autonomous Driving & Intelligent Transport group at Oxford Brookes University, and the OBR\\nAutonomous team for their input and assistance. The authors would also like to thank Salman Khan, Peter Ball,\\nTjeerd Olde Scheper, Matthias Rolf, Alex Rast, and Gordana Collier for their ongoing support.\\nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme, under\\ngrant agreement No. 964505 (E-pi).\\nReferences\\n[1] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. YOLOv4: Optimal Speed and Accuracy of Object\\nDetection. apr 2020.\\n[2] Guimei Cao, Xuemei Xie, Wenzhe Yang, Quan Liao, Guangming Shi, and Jinjian Wu. Feature-fused ssd: Fast detection\\nfor small objects. In Ninth International Conference on Graphic and Image Processing (ICGIP 2017) , volume 10615, page\\n106151E. International Society for Optics and Photonics, 2018.\\n[3] Chenyi Chen, Ming-Yu Liu, Oncel Tuzel, and Jianxiong Xiao. R-CNN for Small Object Detection. Lecture Notes in Computer\\nScience (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics) , 10115 LNCS:214–\\n230, nov 2016.\\n[4] Jacob Culley, Sam Garlick, Enric Gil Esteller, Petar Georgiev, Ivan Fursa, Isaac Vander Sluis, Peter Ball, and Andrew Bradley.\\nSystem design for a driverless autonomous racing vehicle. 2020 12th International Symposium on Communication Systems,\\nNetworks and Digital Signal Processing (CSNDSP) , pages 1–6, 2020.\\n[5] Ross Girshick. Fast r-cnn, 2015.\\n[6] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and\\nsemantic segmentation Tech report (v5).\\n[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial Pyramid Pooling in Deep Convolutional Networks for\\nVisual Recognition. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and\\nLecture Notes in Bioinformatics) , 8691 LNCS(PART 3):346–361, jun 2014.\\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. Proceedings of\\nthe IEEE Computer Society Conference on Computer Vision and Pattern Recognition , 2016-December:770–778, dec 2015.\\n[9] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely Connected Convolutional Networks.\\nTechnical report.', metadata={'source': 'pdfs/2112.11798.pdf', 'page': 9}),\n",
              " Document(page_content=\"[10] Wei Jia, Shiquan Xu, Zhen Liang, Yang Zhao, Hai Min, Shujie Li, and Ye Yu. Real-time automatic helmet detection of\\nmotorcyclists in urban trafﬁc using improved YOLOv5 detector. IET Image Processing , page ipr2.12295, jun 2021.\\n[11] Glenn Jocher, Alex Stoken, Jirka Borovec, NanoCode012, Ayush Chaurasia, TaoXie, Liu Changyu, Abhiram V , Laughing,\\ntkianai, yxNONG, Adam Hogan, lorenzomammana, AlexWang1900, Jan Hajek, Laurentiu Diaconu, Marc, Yonghye Kwon,\\noleg, wanghaoyang0106, Yann Defretin, Aditya Lohia, ml5ah, Ben Milanko, Benjamin Fineran, Daniel Khromov, Ding\\nYiwei, Doug, Durgesh, and Francisco Ingham. ultralytics/yolov5: v5.0 - YOLOv5-P6 1280 models, AWS, Supervise.ly and\\nYouTube integrations, Apr. 2021.\\n[12] Mate Kisantal, Zbigniew Wojna, Jakub Murawski, Jacek Naruniec, and Kyunghyun Cho. Augmentation for small object\\ndetection, 2019.\\n[13] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature Pyramid Networks\\nfor Object Detection. dec 2016.\\n[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan,\\nC. Lawrence Zitnick, and Piotr Doll ´ar. Microsoft COCO: Common Objects in Context. Lecture Notes in Computer Science\\n(including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics) , 8693 LNCS(PART 5):740–\\n755, may 2014.\\n[15] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation, 2018.\\n[16] Yifan Liu, Binghang Lu, Jingyu Peng, and Zihao Zhang. Research on the Use of YOLOv5 Object Detection Algorithm in\\nMask Wearing Recognition. World Scientiﬁc Research Journal , 6:2020.\\n[17] Will Maddern, Geoff Pascoe, Chris Linegar, and Paul Newman. 1 Year, 1000km: The Oxford RobotCar Dataset. The\\nInternational Journal of Robotics Research (IJRR) , 36(1):3–15, 2017.\\n[18] Valentina Mus ,at, Ivan Fursa, Paul Newman, Fabio Cuzzolin, and Andrew Bradley. Multi-weather city: Adverse weather\\nstacking for autonomous driving. Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 2906–\\n2915, 2021.\\n[19] Nhat Duy Nguyen, Tien Do, Thanh Duc Ngo, and Duy Dinh Le. An Evaluation of Deep Learning Methods for Small Object\\nDetection. Journal of Electrical and Computer Engineering , 2020.\\n[20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,\\nNatalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\\nperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett,\\neditors, Advances in Neural Information Processing Systems 32 , pages 8024–8035. Curran Associates, Inc., 2019.\\n[21] Phuoc Pham, Duy Nguyen, Tien Do, Thanh Duc Ngo, and Duy-Dinh Le. Evaluation of Deep Models for Real-Time Small\\nObject Detection. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture\\nNotes in Bioinformatics) , 10636 LNCS:516–526, nov 2017.\\n[22] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/darknet/ , 2013–2016.\\n[23] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You Only Look Once: Uniﬁed, Real-Time Object\\nDetection. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , 2016-\\nDecember:779–788, jun 2015.\\n[24] Joseph Redmon and Ali Farhadi. YOLO9000: Better, Faster, Stronger. Proceedings - 30th IEEE Conference on Computer\\nVision and Pattern Recognition, CVPR 2017 , 2017-January:6517–6525, dec 2016.\\n[25] Joseph Redmon and Ali Farhadi. YOLOv3: An Incremental Improvement. apr 2018.\\n[26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region\\nproposal networks, 2016.\\n[27] Bharat Singh and Larry S. Davis. An Analysis of Scale Invariance in Object Detection SNIP, 2018.\\n[28] Bharat Singh, Mahyar Najibi, and Larry S. Davis. SNIPER: Efﬁcient Multi-Scale Training. Advances in Neural Information\\nProcessing Systems , 2018-December:9310–9320, may 2018.\\n[29] Bharat Singh, Mahyar Najibi, Abhishek Sharma, and Larry S Davis. Scale Normalized Image Pyramids with AutoFocus for\\nObject Detection.\\n[30] Mingxing Tan, Ruoming Pang, and Quoc V . Le. EfﬁcientDet: Scalable and Efﬁcient Object Detection. Proceedings of the\\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 10778–10787, nov 2019.\\n[31] Bichen Wu, Forrest Iandola, Peter H Jin, and Kurt Keutzer. SqueezeDet: Uniﬁed, Small, Low Power Fully Convolutional\\nNeural Networks for Real-Time Object Detection for Autonomous Driving.\\n[32] Bin Yan, Pan Fan, Xiaoyan Lei, Zhijie Liu, and Fuzeng Yang. A Real-Time Apple Targets Detection Method for Picking\\nRobot Based on Improved YOLOv5. Remote Sensing 2021, Vol. 13, Page 1619 , 13(9):1619, apr 2021.\\n[33] Guanhao Yang, Wei Feng, Jintao Jin, Qujiang Lei, Xiuhao Li, Guangchao Gui, and Weijun Wang. Face Mask Recognition\\nSystem with YOLOV5 Based on Image Recognition. 2020 IEEE 6th International Conference on Computer and Communi-\\ncations, ICCC 2020 , pages 1398–1404, dec 2020.\\n[34] Fangbo Zhou, Huailin Zhao, and Zhen Nie. Safety Helmet Detection Based on YOLOv5. Proceedings of 2021 IEEE Inter-\\nnational Conference on Power Electronics, Computer Applications, ICPECA 2021 , pages 6–11, jan 2021.\", metadata={'source': 'pdfs/2112.11798.pdf', 'page': 10})]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXZiDsNzv1u4",
        "outputId": "a5142413-3184-47b7-b10d-4cd32f056640"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "text_chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "j-HBEKVxv3pf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of text chunks\",len(text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC_TANpHwfxA",
        "outputId": "a4b5797b-e4e7-4355-9bd5-bb01638cbeeb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of text chunks 93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "knV-6U5Lwrf5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = embeddings.embed_query(\"Goa yay\")"
      ],
      "metadata": {
        "id": "khenCrU4w0ox"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(query_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T93QKHsExD2U",
        "outputId": "736852ff-0696-40ac-92aa-ffc21844abec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')"
      ],
      "metadata": {
        "id": "kKe7KsqhxH0f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_API_ENV  # next to api key in console\n",
        ")\n",
        "index_name = \"myindex\" # put in the name of your pinecone index here\n"
      ],
      "metadata": {
        "id": "2GUYGCtwyBSO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "U-LGb4CtyTg4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is yolo?\"\n",
        "docs = docsearch.similarity_search(query, k=3)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxkeGnXPymRe",
        "outputId": "369ad935-280f-4d7a-f151-841b88ac27c8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Figure 7: Performance comparison between the YOLOv5 and YOLO-Z families of models, plotting mAP (top), mAP for small objects (middle)\\nand mAP for medium objects (bottom) against inference time (ms). Clearly the superior average performance of YOLO-Z is achieved at the smaller\\nscale, while performance is stable and very close to 1 at the medium scale.\\n4.2 Inﬂuence of neck architecture'),\n",
              " Document(page_content='4.5 Other factors\\nFigure 8: Visual demonstration of the improved detec-\\ntion results of YOLO-Z S (bottom) compared to YOLOv5\\nS (top) over a region of a sample image covering far away /\\nsmall scale objects. Yellow and blue cone detections are\\nshown as bounding boxes in the respective colours, de-\\ntections missed by both models are shown as red boxes,\\ndetections missed by YOLOv5 but correctly identiﬁed by\\nYOLO-Z S as red circles. One can observe that the im-'),\n",
              " Document(page_content='With YOLOv1 [23], object detection is presented as a regression task, thus simplifying the networks and allowing us\\nto build faster models that can be used in real-time. Later versions of YOLO improve different aspects of the model\\n[23, 24, 25, 1]. Most notably, much effort has been spent on the backbone through the different versions. This begs\\nthe question: What potential is there still untapped if changes to an isolated element can have such an impact?')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwogSPnly8_B",
        "outputId": "dc1e3a0d-7159-4d71-bc07-2250f7324b52"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: google-generativeai<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.1.38)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.11.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.23.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (0.1.38)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (23.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (8.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain-google-genai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1->langchain-google-genai) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain-google-genai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain-google-genai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain-google-genai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain-google-genai) (2024.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.62.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('google_api_key')\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "jtavyVCszJSS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                             temperature=0.5,convert_system_message_to_human=True)"
      ],
      "metadata": {
        "id": "b_2ADS780My9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=docsearch.as_retriever()"
      ],
      "metadata": {
        "id": "i8u7ohYt0RZl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever)"
      ],
      "metadata": {
        "id": "9O00pRYD0C3X"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-pQt-dj0xBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])"
      ],
      "metadata": {
        "id": "b5ESmuaC0e1a"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is yolov5\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDDNMcoP1H9s",
        "outputId": "04e4fe9f-db80-440c-d9ec-8e0f0ad0444b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv5 is a real-time object detection model that was released shortly after YOLOv4. It is fully written in the PyTorch framework and has a slightly modified backbone to simplify it, which offers the potential to adapt to the system's requirements and opens the way for additional changes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is YOLO-Z\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj7M1GVJ1TC-",
        "outputId": "6d4aec89-80fe-44cf-b6ce-2842fa5d3316"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO-Z is a family of object detection models that were introduced in the paper \"YOLO-Z: A Novel Architecture for Object Detection\". The YOLO-Z models are based on the YOLOv5 architecture, but they incorporate several new features that improve their performance, including a new neck architecture, a new loss function, and a new training strategy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"compare the performance between yolov5 and yolo-z\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMI4y0Bw2VIp",
        "outputId": "9514257f-b733-4d13-fc9d-52a9ff693491"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure 7 shows a performance comparison between the YOLOv5 and YOLO-Z families of models, plotting mAP (top), mAP for small objects (middle), and mAP for medium objects (bottom) against inference time (ms). Clearly, the superior average performance of YOLO-Z is achieved at the smaller scale, while performance is stable and very close to 1 at the medium scale.\n",
            "\n",
            "Table 2 compares performance and inference time of YOLOv5 and YOLO-Z (optimal values for each scale in bold). Note that the table only shows performance on the yellow and blue classes, as they are the best represented in the dataset according to Figure 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"what is yolov5\"\n",
        "print(qa_chain.invoke(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KVW3XAE0jNE",
        "outputId": "38bfb747-0844-4936-f012-6dee7495e53d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'what is yolov5', 'result': 'YOLOv5 is a real-time object detection system that was released shortly after YOLOv4. It is fully written in the PyTorch framework and shares the same design as YOLOv4, but with a slightly modified backbone that offers the potential for adaptation to specific system requirements.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q&A Using Langchain**"
      ],
      "metadata": {
        "id": "NBo0jCPuDor2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.vectorstores import Chroma\n"
      ],
      "metadata": {
        "id": "iPVe5CyaDwPU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "\n",
        "#llm = (model=\"models/aqa\",temperature=0.5,convert_system_message_to_human=True)"
      ],
      "metadata": {
        "id": "DVptyzJi9Kqc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You are a helpful AI assistant.\n",
        "Answer based on the context provided.\n",
        "context: {context}\n",
        "input: {input}\n",
        "answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n"
      ],
      "metadata": {
        "id": "lsZZ9kPzDmxo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=retrieval_chain.invoke({\"input\":\"compare the performance between yolov5 and yolo-z\"})\n",
        "\n",
        "#Print the answer to the question\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql0id407GKp1",
        "outputId": "05f6c1b1-de4c-449e-e237-3f88126b3d94"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure 7 shows the performance comparison between the YOLOv5 and YOLO-Z families of models. YOLO-Z consistently outperforms YOLOv5 in terms of mAP, mAP for small objects, and mAP for medium objects. This superior performance is particularly evident at the smaller scale, where YOLO-Z achieves a mAP of over 90%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=retrieval_chain.invoke({\"input\":\"how performance of yolov5 can be modified.\"})\n",
        "\n",
        "#Print the answer to the question\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vb6w1JJGZw5",
        "outputId": "77dab195-99d0-4b20-a15f-aeb41a0cdacd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The backbone of YOLOv5 can be modified to simplify it, which offers the potential to adapt to the system's requirements and opens the way for additional changes. If a single backbone element is modified, more drastic changes can be applied for additional effects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=retrieval_chain.invoke({\"input\":\"what does this research paper talks about.\"})\n",
        "\n",
        "#Print the answer to the question\n",
        "print(response[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsLO4L0QIW7-",
        "outputId": "dbab740d-cd3b-4b26-c785-8ec4fc5c2b45"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This research paper focuses on improving the performance of object detection systems in autonomous vehicles, particularly for small objects like cones, which are critical for safe navigation. The paper explores various approaches to enhance detection accuracy and reduce inference time, ultimately aiming to improve the overall safety and efficiency of autonomous vehicles.\n"
          ]
        }
      ]
    }
  ]
}